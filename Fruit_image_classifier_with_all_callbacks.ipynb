{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 67692 images belonging to 131 classes.\n",
      "Found 22688 images belonging to 131 classes.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Activation,Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "\n",
    "num_classes=131\n",
    "img_rows,img_cols=100,100\n",
    "batch_size=16\n",
    "\n",
    "train_data_dir=\"D:/fruits-360/Training/\"\n",
    "validation_data_dir=\"D:/fruits-360/validation/\"\n",
    "\n",
    "train_datagen=ImageDataGenerator(rescale=1./255,\n",
    "                                 rotation_range=30,\n",
    "                                 width_shift_range=0.3,\n",
    "                                 horizontal_flip=True,\n",
    "                                 fill_mode='nearest')\n",
    "\n",
    "validation_datagen=ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator=train_datagen.flow_from_directory(train_data_dir,\n",
    "                                                 target_size=(img_rows, img_cols),\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 class_mode=\"categorical\",\n",
    "                                                 shuffle=True)\n",
    "\n",
    "validation_generator=validation_datagen.flow_from_directory(validation_data_dir,\n",
    "                                                           target_size=(img_rows, img_cols),\n",
    "                                                           batch_size=batch_size,\n",
    "                                                           class_mode=\"categorical\",\n",
    "                                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\test\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 100, 100, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 100, 100, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 98, 98, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 98, 98, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 49, 49, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 49, 49, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 49, 49, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 49, 49, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 47, 47, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 47, 47, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 23, 23, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 23, 23, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 33856)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               17334784  \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 131)               67203     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 131)               0         \n",
      "=================================================================\n",
      "Total params: 17,467,555\n",
      "Trainable params: 17,467,555\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#lets define our model\n",
    "model=Sequential()\n",
    "\n",
    "model.add(Conv2D(32,(3,3),padding='same',\n",
    "                input_shape=(img_rows,img_cols,3)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Conv2D(32,(3,3)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64,(3,3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64,(3,3)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#below can be done but gives error, maybe version issue can be fixed later\n",
    "#from keras.utils.vis_utils import plot_model\n",
    "#plot_model(model,show_shapes=True,show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\test\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/3\n",
      "43/43 [==============================] - 714s 17s/step - loss: 7.7198 - accuracy: 0.0102 - val_loss: 5.1768 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.17676, saving model to D:/backup for python files/fruit_image_classifier_with_all_callbacks.h5\n",
      "Epoch 2/3\n",
      "43/43 [==============================] - 634s 15s/step - loss: 4.8800 - accuracy: 0.0087 - val_loss: 4.8151 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_loss improved from 5.17676 to 4.81506, saving model to D:/backup for python files/fruit_image_classifier_with_all_callbacks.h5\n",
      "Epoch 3/3\n",
      "43/43 [==============================] - 640s 15s/step - loss: 4.8719 - accuracy: 0.0131 - val_loss: 4.6649 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.81506 to 4.66491, saving model to D:/backup for python files/fruit_image_classifier_with_all_callbacks.h5\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\n",
    "\n",
    "checkpoint=ModelCheckpoint(r\"D:/backup for python files/fruit_image_classifier_with_all_callbacks.h5\",\n",
    "                          monitor=\"val_loss\",\n",
    "                          mode=\"min\",\n",
    "                          save_best_only=True,\n",
    "                          verbose=1)\n",
    "\n",
    "earlystop=EarlyStopping(monitor=\"val_loss\",\n",
    "                       min_delta=0,\n",
    "                       patience=3,\n",
    "                       verbose=1,\n",
    "                       restore_best_weights=True)\n",
    "\n",
    "reduce_lr=ReduceLROnPlateau(monitor=\"val_loss\",\n",
    "                           factor=0.2,\n",
    "                           patience=3,\n",
    "                           min_delta=0.0001)\n",
    "\n",
    "callbacks=[checkpoint,earlystop,reduce_lr]\n",
    "\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "             optimizer=Adam(learning_rate=0.01),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "nb_train_samples=692\n",
    "nb_validation_samples=688\n",
    "epochs=3\n",
    "\n",
    "\n",
    "history=model.fit_generator(train_generator,\n",
    "                           steps_per_epoch=nb_train_samples // batch_size,\n",
    "                           epochs=epochs,\n",
    "                           callbacks=callbacks,\n",
    "                           validation_data=validation_generator,\n",
    "                           validation_steps=nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.23.1-cp36-cp36m-win_amd64.whl (6.8 MB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\programdata\\anaconda3\\envs\\test\\lib\\site-packages (from scikit-learn) (1.5.0)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-0.16.0-py3-none-any.whl (300 kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\programdata\\anaconda3\\envs\\test\\lib\\site-packages (from scikit-learn) (1.18.5)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-0.16.0 scikit-learn-0.23.1 threadpoolctl-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##DISPLAYING OUR CONFUSION MATRIX\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "img_row,img_height,img_depth=32,32,3\n",
    "model=load_model(\"D:/backup for python files/fruit_image_classifier_with_all_callbacks.h5\")\n",
    "\n",
    "class_labels=validation_generator.class_indices\n",
    "class_labels={v:k for k, v in class_labels.items() }\n",
    "classes=list(class_labels.values())\n",
    "\n",
    "nb_train_samples=692 \n",
    "nb_validation_samples=688 \n",
    "\n",
    "#confusion matrix and classification report\n",
    "Y_pred=model.predict_generator(validation_generator,nb_validation_samples //batch_size)\n",
    "y_pred=np.argmax(Y_pred,axis=1)\n",
    "\n",
    "target_names=list(class_labels.values())\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "cnf_matrix=confusion_matrix(validation_generator.classes, y_pred)\n",
    "\n",
    "plt.imshow(cnf_matrix,interpolation='nearest')\n",
    "plt.colorbar()\n",
    "tick_marks=np.arange(len(classes))\n",
    "_=plt.xticks(tick_marks,classes, rotation=90)\n",
    "_=plt.yticks(tick_names,classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "filter expected 2 arguments, got 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-c01b8e2e59a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"D:/fruits-360/validation/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_label\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mgetRandomImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_height\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[0mfiles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mtrue_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrue_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-c01b8e2e59a5>\u001b[0m in \u001b[0;36mgetRandomImage\u001b[1;34m(pathj, img_width, img_height)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgetRandomImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpathj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_height\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;34m\"\"\"function loads a random images from a random folder in our test path\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mfolders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mrandom_directory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mpath_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfolders\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrandom_directory\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: filter expected 2 arguments, got 1"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "\n",
    "def draw_test(name,pred,im,true_label):\n",
    "    BLACK=[0,0,0]\n",
    "    expanded_image=cv2.copyMakeBorder(im,160,0,0,500,cv2.BORDER_CONSTANT,value=BLACK)\n",
    "    cv2.putText(expanded_image, \"predicted-\"+pred, (20,60),cv2.FONT_HERSHEY_SIMPLES,1,(0,0,255),2)\n",
    "    cv2.putText(expanded_image,\"true-\"+true_label,(20,120),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)\n",
    "    cv2.imshow(name,expanded_image)\n",
    "    \n",
    "    \n",
    "\n",
    "def getRandomImage(pathj, img_width, img_height):\n",
    "    \"\"\"function loads a random images from a random folder in our test path\"\"\"\n",
    "    folders=list(filter(lambda x: os.path.join(path,x)), os.listdir(path))\n",
    "    random_directory=np.random.randint(0,len(folders))\n",
    "    path_class=folders[random_directory]\n",
    "    file_path=path+path_class\n",
    "    file_names=[f for f in listdir(file_path) if isfile(join(file_path,f))]\n",
    "    random_file_index=np.random.randint(0,len(file_names))\n",
    "    image_name=file_names[random_file_index]\n",
    "    final_path=file_path+\"/\"+image_name\n",
    "    return image.load_img(final_path,target_size=(img_width,imgheight)),final_path,path_class\n",
    "\n",
    "#dimensions of our images\n",
    "img_width, img_height= 32,32\n",
    "\n",
    "files=[]\n",
    "predictions=[]\n",
    "true_labels=[]\n",
    "\n",
    "#predicting images\n",
    "\n",
    "for i in range(0,10):\n",
    "    path=\"D:/fruits-360/validation/\"\n",
    "    img, final_path, true_label= getRandomImage(path, img_width, img_height)\n",
    "    files.append(final_path)\n",
    "    true_labels.append(true_label)\n",
    "    x=image.img_to_array(img)\n",
    "    x=x*1./255\n",
    "    x=np.expand_dims(x,axis=0)\n",
    "    images=np.vstack([x])\n",
    "    classes=model.predict_classes(iamges,batch_size=10)\n",
    "    predictions.append(classes)\n",
    "    \n",
    "    \n",
    "for i in range(0,len(files)):\n",
    "    image=cv2.imread((files[i]))\n",
    "    draw_test(\"prediction\",class_labels[predictions[i][0]],image,true_labels[i])\n",
    "    cv2.waitkey(0)\n",
    "    \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
