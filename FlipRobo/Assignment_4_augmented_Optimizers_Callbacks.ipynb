{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Dropout, Conv2D,MaxPooling2D,Activation,Flatten,ZeroPadding2D\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=\"C:/Users/Shailendra Yadav/Datasets/Assignment2/training1/jeans/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy\n",
    "import cv2\n",
    "mypath='C:/Users/Shailendra Yadav/Datasets/Assignment2/training1/jeans/' #here give path according to where your training data is stored\n",
    "\n",
    "onlyfiles = [ f for f in listdir(mypath) if isfile(join(mypath,f)) ]\n",
    "images = numpy.empty(len(onlyfiles), dtype=object)\n",
    "for n in range(0, len(onlyfiles)):\n",
    "  images[n] = cv2.imread( join(mypath,onlyfiles[n]) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "print(len(onlyfiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "for j,i in enumerate(onlyfiles):\n",
    "    image=cv2.imread('C:/Users/Shailendra Yadav/Datasets/Assignment2/training1/jeans/'+i)\n",
    "    height,width=image.shape[:2]\n",
    "    rotation_matrix=cv2.getRotationMatrix2D((width/2,height/2),30,.5)\n",
    "    rotated_image=cv2.warpAffine(image, rotation_matrix,(width,height))\n",
    "    # in the below line corrected_code_augmented is the folder where we are saving the augmented images. so first create a directory \n",
    "    # like this and then the path as shown below.\n",
    "    cv2.imwrite('D:/fliprobo/augmented_rotated(30)/jeans/'+'img{}.jpg'.format(j),rotated_image)\n",
    "    \n",
    "    \n",
    "\n",
    "    l=[]\n",
    "for j,i in enumerate(onlyfiles):\n",
    "    image=cv2.imread('C:/Users/Shailendra Yadav/Datasets/Assignment2/training1/trouser/'+i)\n",
    "    height,width=image.shape[:2]\n",
    "    rotation_matrix=cv2.getRotationMatrix2D((width/2,height/2),30,.5)\n",
    "    rotated_image=cv2.warpAffine(image, rotation_matrix,(width,height))\n",
    "    # in the below line corrected_code_augmented is the folder where we are saving the augmented images. so first create a directory \n",
    "    # like this and then the path as shown below.\n",
    "    cv2.imwrite('D:/fliprobo/augmented_rotated(30)/trouser/'+'img{}.jpg'.format(j),rotated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "for j,i in enumerate(onlyfiles):\n",
    "    image=cv2.imread('C:/Users/Shailendra Yadav/Datasets/Assignment2/training1/jeans/'+i)\n",
    "    height,width=image.shape[:2]\n",
    "    rotation_matrix=cv2.getRotationMatrix2D((width/2,height/2),-30,.5)\n",
    "    rotated_image=cv2.warpAffine(image, rotation_matrix,(width,height))\n",
    "    # in the below line corrected_code_augmented is the folder where we are saving the augmented images. so first create a directory \n",
    "    # like this and then the path as shown below.\n",
    "    cv2.imwrite('D:/fliprobo/augmented_rotated(-30)/jeans/'+'img{}.jpg'.format(j),rotated_image)\n",
    "    \n",
    "    \n",
    "l=[]\n",
    "for j,i in enumerate(onlyfiles):\n",
    "    image=cv2.imread('C:/Users/Shailendra Yadav/Datasets/Assignment2/training1/trouser/'+i)\n",
    "    height,width=image.shape[:2]\n",
    "    rotation_matrix=cv2.getRotationMatrix2D((width/2,height/2),-30,.5)\n",
    "    rotated_image=cv2.warpAffine(image, rotation_matrix,(width,height))\n",
    "    # in the below line corrected_code_augmented is the folder where we are saving the augmented images. so first create a directory \n",
    "    # like this and then the path as shown below.\n",
    "    cv2.imwrite('D:/fliprobo/augmented_rotated(-30)/trouser/'+'img{}.jpg'.format(j),rotated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "for j,i in enumerate(onlyfiles):\n",
    "    image=cv2.imread('C:/Users/Shailendra Yadav/Datasets/Assignment2/training1/jeans/'+i)\n",
    "    \n",
    "    M=np.ones(image.shape, dtype='uint8')*75\n",
    "    added=cv2.add(image,M)\n",
    "    # in the below line corrected_code_augmented is the folder where we are saving the augmented images. so first create a directory \n",
    "    # like this and then the path as shown below.\n",
    "    cv2.imwrite('D:/fliprobo/Brightness/jeans/'+'img{}.jpg'.format(j),added)\n",
    "    \n",
    "    \n",
    "l=[]\n",
    "for j,i in enumerate(onlyfiles):\n",
    "    image=cv2.imread('C:/Users/Shailendra Yadav/Datasets/Assignment2/training1/trouser/'+i)\n",
    "    \n",
    "    M=np.ones(image.shape, dtype='uint8')*75\n",
    "    added=cv2.add(image,M)\n",
    "    # in the below line corrected_code_augmented is the folder where we are saving the augmented images. so first create a directory \n",
    "    # like this and then the path as shown below.\n",
    "    cv2.imwrite('D:/fliprobo/Brightness/trouser/'+'img{}.jpg'.format(j),added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "for j,i in enumerate(onlyfiles):\n",
    "    image=cv2.imread('C:/Users/Shailendra Yadav/Datasets/Assignment2/training1/jeans/'+i)\n",
    "    \n",
    "    M=np.ones(image.shape, dtype='uint8')*75\n",
    "    subtracted=cv2.subtract(image,M)\n",
    "    # in the below line corrected_code_augmented is the folder where we are saving the augmented images. so first create a directory \n",
    "    # like this and then the path as shown below.\n",
    "    cv2.imwrite('D:/fliprobo/low_brightness/jeans/'+'img{}.jpg'.format(j),subtracted)\n",
    "    \n",
    "l=[]\n",
    "for j,i in enumerate(onlyfiles):\n",
    "    image=cv2.imread('C:/Users/Shailendra Yadav/Datasets/Assignment2/training1/trouser/'+i)\n",
    "    \n",
    "    M=np.ones(image.shape, dtype='uint8')*75\n",
    "    subtracted=cv2.subtract(image,M)\n",
    "    # in the below line corrected_code_augmented is the folder where we are saving the augmented images. so first create a directory \n",
    "    # like this and then the path as shown below.\n",
    "    cv2.imwrite('D:/fliprobo/low_brightness/trouser/'+'img{}.jpg'.format(j),subtracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "for j,i in enumerate(onlyfiles):\n",
    "    image=cv2.imread('C:/Users/Shailendra Yadav/Datasets/Assignment2/training1/jeans/'+i)\n",
    "    \n",
    "    flipped=cv2.flip(image,1)\n",
    "    # in the below line corrected_code_augmented is the folder where we are saving the augmented images. so first create a directory \n",
    "    # like this and then the path as shown below.\n",
    "    cv2.imwrite('D:/fliprobo/flipped/jeans/'+'img{}.jpg'.format(j),flipped)\n",
    "    \n",
    "l=[]\n",
    "for j,i in enumerate(onlyfiles):\n",
    "    image=cv2.imread('C:/Users/Shailendra Yadav/Datasets/Assignment2/training1/trouser/'+i)\n",
    "    \n",
    "    flipped=cv2.flip(image,1)\n",
    "    # in the below line corrected_code_augmented is the folder where we are saving the augmented images. so first create a directory \n",
    "    # like this and then the path as shown below.\n",
    "    cv2.imwrite('D:/fliprobo/flipped/trouser/'+'img{}.jpg'.format(j),flipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "for j,i in enumerate(onlyfiles):\n",
    "    image=cv2.imread('C:/Users/Shailendra Yadav/Datasets/Assignment2/training1/jeans/'+i)\n",
    "    \n",
    "    \n",
    "    kernel_sharpening=np.array([[-1,-1,-1],\n",
    "                            [-1,10,-1],\n",
    "                            [-1,-1,-1]])\n",
    "#applying different kernels to the input image\n",
    "    sharpened=cv2.filter2D(image,-1,kernel_sharpening)\n",
    "    # in the below line corrected_code_augmented is the folder where we are saving the augmented images. so first create a directory \n",
    "    # like this and then the path as shown below.\n",
    "    cv2.imwrite('D:/fliprobo/sharpened/jeans/'+'img{}.jpg'.format(j),sharpened)\n",
    "    \n",
    "    \n",
    "l=[]\n",
    "for j,i in enumerate(onlyfiles):\n",
    "    image=cv2.imread('C:/Users/Shailendra Yadav/Datasets/Assignment2/training1/trouser/'+i)\n",
    "    \n",
    "    \n",
    "    kernel_sharpening=np.array([[-1,-1,-1],\n",
    "                            [-1,10,-1],\n",
    "                            [-1,-1,-1]])\n",
    "#applying different kernels to the input image\n",
    "    sharpened=cv2.filter2D(image,-1,kernel_sharpening)\n",
    "    # in the below line corrected_code_augmented is the folder where we are saving the augmented images. so first create a directory \n",
    "    # like this and then the path as shown below.\n",
    "    cv2.imwrite('D:/fliprobo/sharpened/trouser/'+'img{}.jpg'.format(j),sharpened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=\"D:/fliprobo/training1/\"\n",
    "validation=\"D:/fliprobo/validation1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=(150,150,3)\n",
    "img_width=150\n",
    "img_height=150\n",
    "\n",
    "train_samples=490\n",
    "validation_samples=60\n",
    "\n",
    "batch_size=32\n",
    "num_classes=2\n",
    "epochs=10\n",
    "\n",
    "datagen=ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 980 images belonging to 2 classes.\n",
      "Found 60 images belonging to 2 classes.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\test\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 148, 148, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 72, 72, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 72, 72, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 34, 34, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 34, 34, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18496)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                1183808   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,212,513\n",
      "Trainable params: 1,212,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "train_generator=datagen.flow_from_directory(train,\n",
    "                                            target_size=(img_width,img_height),\n",
    "                                           batch_size=16,\n",
    "                                           class_mode='binary')\n",
    "validation_generator=datagen.flow_from_directory(validation,\n",
    "                                                target_size=(img_width,img_height),\n",
    "                                                batch_size=32,\n",
    "                                                class_mode='binary')\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Conv2D(32,(3,3),input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(32,(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(64,(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\n",
    "checkpoint=ModelCheckpoint(r\"D:/backup for python files/Assignment_4_classifier_with_all_callbacks.h5\",\n",
    "                          monitor=\"val_loss\",\n",
    "                          mode=\"min\",\n",
    "                          save_best_only=True,\n",
    "                          verbose=1)\n",
    "\n",
    "earlystop=EarlyStopping(monitor=\"val_loss\",\n",
    "                       min_delta=0,\n",
    "                       patience=3,\n",
    "                       verbose=1,\n",
    "                       restore_best_weights=True)\n",
    "\n",
    "reduce_lr=ReduceLROnPlateau(monitor=\"val_loss\",\n",
    "                           factor=0.2,\n",
    "                           patience=3,\n",
    "                           min_delta=0.0001)\n",
    "\n",
    "callbacks=[checkpoint,earlystop,reduce_lr]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD\n",
    "SGD=SGD(momentum=0.01, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='SGD',\n",
    "             metrics=['accuracy'])\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\test\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/10\n",
      "15/15 [==============================] - 64s 4s/step - loss: 0.7421 - accuracy: 0.5375 - val_loss: 0.6921 - val_accuracy: 0.5312\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.69211, saving model to D:/backup for python files/Assignment_4_classifier_with_all_callbacks.h5\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 65s 4s/step - loss: 0.6946 - accuracy: 0.5132 - val_loss: 0.6928 - val_accuracy: 0.5357\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.69211\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 80s 5s/step - loss: 0.7081 - accuracy: 0.4667 - val_loss: 0.6931 - val_accuracy: 0.4688\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.69211\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 80s 5s/step - loss: 0.6931 - accuracy: 0.5375 - val_loss: 0.6916 - val_accuracy: 0.6429\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.69211 to 0.69156, saving model to D:/backup for python files/Assignment_4_classifier_with_all_callbacks.h5\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 69s 5s/step - loss: 0.6894 - accuracy: 0.5542 - val_loss: 0.6940 - val_accuracy: 0.4375\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.69156\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 64s 4s/step - loss: 0.7037 - accuracy: 0.5083 - val_loss: 0.6936 - val_accuracy: 0.5357\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.69156\n",
      "Epoch 7/10\n",
      "15/15 [==============================] - 61s 4s/step - loss: 0.6933 - accuracy: 0.5439 - val_loss: 0.6944 - val_accuracy: 0.4688\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.69156\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "source": [
    "history=model.fit_generator(train_generator,\n",
    "                            steps_per_epoch=train_samples // batch_size,\n",
    "                 epochs=epochs,\n",
    "                            callbacks=callbacks,\n",
    "                            validation_data=validation_generator,\n",
    "                            validation_steps=validation_samples // batch_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='rmsprop',\n",
    "             metrics=['accuracy'])\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "15/15 [==============================] - 61s 4s/step - loss: 0.8412 - accuracy: 0.4693 - val_loss: 0.6930 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.69156\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 63s 4s/step - loss: 0.6939 - accuracy: 0.4958 - val_loss: 0.6947 - val_accuracy: 0.4643\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.69156\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 65s 4s/step - loss: 0.7011 - accuracy: 0.5333 - val_loss: 0.6906 - val_accuracy: 0.6250\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.69156 to 0.69061, saving model to D:/backup for python files/Assignment_4_classifier_with_all_callbacks.h5\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 73s 5s/step - loss: 0.6814 - accuracy: 0.5333 - val_loss: 0.6955 - val_accuracy: 0.5357\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.69061\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 74s 5s/step - loss: 0.6570 - accuracy: 0.6250 - val_loss: 0.6734 - val_accuracy: 0.6250\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.69061 to 0.67344, saving model to D:/backup for python files/Assignment_4_classifier_with_all_callbacks.h5\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 61s 4s/step - loss: 0.6282 - accuracy: 0.6447 - val_loss: 0.7287 - val_accuracy: 0.4286\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.67344\n",
      "Epoch 7/10\n",
      "15/15 [==============================] - 79s 5s/step - loss: 0.5534 - accuracy: 0.7083 - val_loss: 0.6791 - val_accuracy: 0.6562\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.67344\n",
      "Epoch 8/10\n",
      "15/15 [==============================] - 80s 5s/step - loss: 0.6084 - accuracy: 0.6792 - val_loss: 0.6273 - val_accuracy: 0.6429\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.67344 to 0.62730, saving model to D:/backup for python files/Assignment_4_classifier_with_all_callbacks.h5\n",
      "Epoch 9/10\n",
      "15/15 [==============================] - 80s 5s/step - loss: 0.5790 - accuracy: 0.6958 - val_loss: 0.6102 - val_accuracy: 0.6875\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.62730 to 0.61019, saving model to D:/backup for python files/Assignment_4_classifier_with_all_callbacks.h5\n",
      "Epoch 10/10\n",
      "15/15 [==============================] - 79s 5s/step - loss: 0.5323 - accuracy: 0.7375 - val_loss: 0.6477 - val_accuracy: 0.6786\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.61019\n"
     ]
    }
   ],
   "source": [
    "history=model.fit_generator(train_generator,\n",
    "                            steps_per_epoch=train_samples // batch_size,\n",
    "                 epochs=epochs,\n",
    "                            callbacks=callbacks,\n",
    "                            validation_data=validation_generator,\n",
    "                            validation_steps=validation_samples // batch_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='Adam',\n",
    "             metrics=['accuracy'])\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "15/15 [==============================] - 81s 5s/step - loss: 0.4865 - accuracy: 0.7542 - val_loss: 0.5971 - val_accuracy: 0.6875\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.61019 to 0.59712, saving model to D:/backup for python files/Assignment_4_classifier_with_all_callbacks.h5\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 79s 5s/step - loss: 0.4689 - accuracy: 0.7763 - val_loss: 0.6405 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.59712\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 65s 4s/step - loss: 0.4531 - accuracy: 0.7667 - val_loss: 0.6287 - val_accuracy: 0.6250\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.59712\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 65s 4s/step - loss: 0.4352 - accuracy: 0.7958 - val_loss: 0.7443 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.59712\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00004: early stopping\n"
     ]
    }
   ],
   "source": [
    "history=model.fit_generator(train_generator,\n",
    "                            steps_per_epoch=train_samples // batch_size,\n",
    "                 epochs=epochs,\n",
    "                            callbacks=callbacks,\n",
    "                            validation_data=validation_generator,\n",
    "                            validation_steps=validation_samples // batch_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Nadam\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='Nadam',\n",
    "             metrics=['accuracy'])\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "15/15 [==============================] - 66s 4s/step - loss: 0.5008 - accuracy: 0.7333 - val_loss: 0.7343 - val_accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 262s 17s/step - loss: 0.4702 - accuracy: 0.7667 - val_loss: 0.7956 - val_accuracy: 0.6071\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.59712\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 65s 4s/step - loss: 0.5137 - accuracy: 0.7544 - val_loss: 0.6077 - val_accuracy: 0.6875\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.59712\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 66s 4s/step - loss: 0.5126 - accuracy: 0.7208 - val_loss: 0.6693 - val_accuracy: 0.6786\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.59712\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 65s 4s/step - loss: 0.5506 - accuracy: 0.6917 - val_loss: 0.6138 - val_accuracy: 0.6562\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.59712\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 63s 4s/step - loss: 0.4393 - accuracy: 0.7750 - val_loss: 1.0367 - val_accuracy: 0.4643\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.59712\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "history=model.fit_generator(train_generator,\n",
    "                            steps_per_epoch=train_samples // batch_size,\n",
    "                 epochs=epochs,\n",
    "                            callbacks=callbacks,\n",
    "                            validation_data=validation_generator,\n",
    "                            validation_steps=validation_samples // batch_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Assignment4.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dir_path='C:/Users/Shailendra Yadav/Datasets/Assignment2/test1/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['img_01.jpeg', 'img_02.jpeg', 'img_03.jpeg', 'img_101.jpeg', 'img_102.jpeg', 'img_103.jpeg', 'img_11.jpeg', 'img_12.jpeg', 'img_13.jpeg', 'img_21.jpeg', 'img_22.jpeg', 'img_23.jpeg', 'img_232.jpeg', 'img_233.jpeg', 'img_241.jpeg', 'img_242.jpeg', 'img_243.jpeg', 'img_251.jpeg', 'img_252.jpeg', 'img_253.jpeg', 'img_261.jpeg', 'img_262.jpeg', 'img_263.jpeg', 'img_271.jpeg', 'img_272.jpeg', 'img_273.jpeg', 'img_281.jpeg', 'img_282.jpeg', 'img_283.jpeg', 'img_291.jpeg', 'img_292.jpeg', 'img_293.jpeg', 'img_301.jpeg', 'img_302.jpeg', 'img_303.jpeg', 'img_31.jpeg', 'img_311.jpeg', 'img_312.jpeg', 'img_313.jpeg', 'img_32.jpeg', 'img_321.jpeg', 'img_322.jpeg', 'img_323.jpeg', 'img_33.jpeg', 'img_331.jpeg', 'img_41.jpeg', 'img_42.jpeg', 'img_43.jpeg', 'img_51.jpeg', 'img_52.jpeg', 'img_53.jpeg', 'img_61.jpeg', 'img_62.jpeg', 'img_63.jpeg', 'img_71.jpeg', 'img_72.jpeg', 'img_73.jpeg', 'img_81.jpeg', 'img_82.jpeg', 'img_83.jpeg', 'img_91.jpeg', 'img_92.jpeg', 'img_93.jpeg']\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "onlyfiles=[f for f in listdir(predict_dir_path) if isfile(join(predict_dir_path, f))]\n",
    "print(onlyfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_01.jpeg:jean\n",
      "img_02.jpeg:jean\n",
      "img_03.jpeg:trouser\n",
      "img_101.jpeg:jean\n",
      "img_102.jpeg:jean\n",
      "img_103.jpeg:jean\n",
      "img_11.jpeg:trouser\n",
      "img_12.jpeg:jean\n",
      "img_13.jpeg:jean\n",
      "img_21.jpeg:trouser\n",
      "img_22.jpeg:jean\n",
      "img_23.jpeg:jean\n",
      "img_232.jpeg:trouser\n",
      "img_233.jpeg:trouser\n",
      "img_241.jpeg:trouser\n",
      "img_242.jpeg:trouser\n",
      "img_243.jpeg:trouser\n",
      "img_251.jpeg:jean\n",
      "img_252.jpeg:jean\n",
      "img_253.jpeg:jean\n",
      "img_261.jpeg:trouser\n",
      "img_262.jpeg:trouser\n",
      "img_263.jpeg:trouser\n",
      "img_271.jpeg:trouser\n",
      "img_272.jpeg:trouser\n",
      "img_273.jpeg:trouser\n",
      "img_281.jpeg:trouser\n",
      "img_282.jpeg:trouser\n",
      "img_283.jpeg:trouser\n",
      "img_291.jpeg:trouser\n",
      "img_292.jpeg:trouser\n",
      "img_293.jpeg:trouser\n",
      "img_301.jpeg:jean\n",
      "img_302.jpeg:jean\n",
      "img_303.jpeg:jean\n",
      "img_31.jpeg:jean\n",
      "img_311.jpeg:jean\n",
      "img_312.jpeg:jean\n",
      "img_313.jpeg:jean\n",
      "img_32.jpeg:jean\n",
      "img_321.jpeg:trouser\n",
      "img_322.jpeg:trouser\n",
      "img_323.jpeg:trouser\n",
      "img_33.jpeg:jean\n",
      "img_331.jpeg:jean\n",
      "img_41.jpeg:jean\n",
      "img_42.jpeg:trouser\n",
      "img_43.jpeg:jean\n",
      "img_51.jpeg:jean\n",
      "img_52.jpeg:jean\n",
      "img_53.jpeg:jean\n",
      "img_61.jpeg:jean\n",
      "img_62.jpeg:trouser\n",
      "img_63.jpeg:trouser\n",
      "img_71.jpeg:jean\n",
      "img_72.jpeg:trouser\n",
      "img_73.jpeg:jean\n",
      "img_81.jpeg:jean\n",
      "img_82.jpeg:jean\n",
      "img_83.jpeg:jean\n",
      "img_91.jpeg:jean\n",
      "img_92.jpeg:jean\n",
      "img_93.jpeg:jean\n",
      "total jeans: 36\n",
      "total trousers: 27\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "jean_counter=0\n",
    "trouser_counter=0\n",
    "for file in onlyfiles:\n",
    "    img=image.load_img(predict_dir_path + file,target_size=(img_width,img_height))\n",
    "    x=image.img_to_array(img)\n",
    "    x=np.expand_dims(x, axis=0)\n",
    "    \n",
    "    images=np.vstack([x])\n",
    "    classes=model.predict_classes(images,batch_size=10)\n",
    "    classes=classes[0][0]\n",
    "    \n",
    "    if classes==0:\n",
    "        print(file+\":\"+'jean')\n",
    "        jean_counter+=1\n",
    "    else:\n",
    "        print(file+\":\"+'trouser')\n",
    "        trouser_counter+=1\n",
    "print(\"total jeans:\",jean_counter)\n",
    "print(\"total trousers:\",trouser_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img_to_array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-f121282c98b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mpredict_dir_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict_dir_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Users/Shailendra Yadav/Assignment3.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'img_to_array' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.preprocessing import image\n",
    "predict_dir_path=img_to_array(predict_dir_path)\n",
    "\n",
    "classifier=load_model(\"C:/Users/Shailendra Yadav/Assignment3.h5\")\n",
    "\n",
    "def draw_test(name, pred, input_im):\n",
    "    BLACK=[0,0,0]\n",
    "    if pred==\"[0]\":\n",
    "        pred=\"jeans\"\n",
    "    if pred==\"[1]\":\n",
    "        pred=\"trouser\"\n",
    "    expanded_image=cv2.copyMakeBorder(input_im, 0, 0, 0, imageL.shape[0],cv2.BORDER_CONSTANT,value=BLACK)\n",
    "    \n",
    "    cv2.putText(expanded_image,str(pred),(252,70),cv2.FONT_HERSHEY_COMPLES_SMALL,4,(0,255,0),2)\n",
    "    cv2.imshow(name,expanded_image)\n",
    "    \n",
    "for i in range (0,10):\n",
    "    rand=np.random.randint(0,len(predict_dir_path))\n",
    "    input_im=predict_dir_path[rand]\n",
    "    \n",
    "    imageL=cv2.resize(input_im, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n",
    "    cv2.imshow(\"Test Image\", imageL)\n",
    "    \n",
    "    input_im=input_im.reshape(1,150,150,3)\n",
    "    \n",
    "    ##get prediction\n",
    "    res=str(classifier.predict_classes(input_im,1,verbose=0)[0])\n",
    "    \n",
    "    draw_test(\"prediction\",res,imageL)\n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
