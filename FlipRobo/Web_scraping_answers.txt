1-b
2-c
3-d
4-b
5-c
6-c
7-d
8-c
9-d
10-a,b,d
11-Though sometimes the two terms are used interchangeably the main difference is that web crawlers usually focus on indexing the web while web scrapers extract or "scrape" data from webpages.
12-A robots.txt file tells search engine crawlers which pages or files the crawler can or can't request from your site. This is used mainly to avoid overloading your site with requests; it is not a mechanism for keeping a web page out of Google.
robots.txt is used primarily to manage crawler traffic to your site, and usually to keep a page off Google, depending on the file type

13-"Static" means unchanged or constant, while "dynamic" means changing or lively. Therefore, static Web pages contain the same prebuilt content each time the page is loaded, while the content of dynamic Web pages can be generated on-the-fly.

14-from urllib.request import urlopen
from urllib.error import HTTPError
from bs4 import BeautifulSoup
def getTitle(url):
    try:
        html = urlopen(url)
    except HTTPError as e:
        return None
    try:
        bsObj = BeautifulSoup(html.read(), "lxml")
        title = bsObj.body.h1
    except AttributeError as e:
        return None
    return title
    
    title = getTitle(url)
    if title == None:
      return "Title could not be found"
    else:
      return title

print(getTitle("https://www.w3resource.com/"))
print(getTitle("http://www.example.com/"))


15-from selenium import webdriver 
  
search_string = input("Input the URL or string you want to search for:") 
   
search_string = search_string.replace(' ', '+')  
  
browser = webdriver.Chrome('chromedriver') 
  
for i in range(1): 
    matched_elements = browser.get("https://www.google.com/search?q=" +
                                     search_string + "&start=" + str(i)) 