{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Dropout, Conv2D,MaxPooling2D,Activation,Flatten,ZeroPadding2D\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=\"C:/Users/Shailendra Yadav/Datasets/Assignment2/training1/jeans/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy\n",
    "import cv2\n",
    "mypath='C:/Users/Shailendra Yadav/Datasets/Assignment2/training1/jeans/' #here give path according to where your training data is stored\n",
    "\n",
    "onlyfiles = [ f for f in listdir(mypath) if isfile(join(mypath,f)) ]\n",
    "images = numpy.empty(len(onlyfiles), dtype=object)\n",
    "for n in range(0, len(onlyfiles)):\n",
    "  images[n] = cv2.imread( join(mypath,onlyfiles[n]) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "print(len(onlyfiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "for j,i in enumerate(onlyfiles):\n",
    "    image=cv2.imread('C:/Users/Shailendra Yadav/Datasets/Assignment2/training1/jeans/'+i)\n",
    "    height,width=image.shape[:2]\n",
    "    rotation_matrix=cv2.getRotationMatrix2D((width/2,height/2),30,.5)\n",
    "    rotated_image=cv2.warpAffine(image, rotation_matrix,(width,height))\n",
    "    # in the below line corrected_code_augmented is the folder where we are saving the augmented images. so first create a directory \n",
    "    # like this and then the path as shown below.\n",
    "    cv2.imwrite('D:/fliprobo/augmented_rotated(30)/jeans/'+'img{}.jpg'.format(j),rotated_image)\n",
    "    \n",
    "    \n",
    "\n",
    "    l=[]\n",
    "for j,i in enumerate(onlyfiles):\n",
    "    image=cv2.imread('C:/Users/Shailendra Yadav/Datasets/Assignment2/training1/trouser/'+i)\n",
    "    height,width=image.shape[:2]\n",
    "    rotation_matrix=cv2.getRotationMatrix2D((width/2,height/2),30,.5)\n",
    "    rotated_image=cv2.warpAffine(image, rotation_matrix,(width,height))\n",
    "    # in the below line corrected_code_augmented is the folder where we are saving the augmented images. so first create a directory \n",
    "    # like this and then the path as shown below.\n",
    "    cv2.imwrite('D:/fliprobo/augmented_rotated(30)/trouser/'+'img{}.jpg'.format(j),rotated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "for j,i in enumerate(onlyfiles):\n",
    "    image=cv2.imread('C:/Users/Shailendra Yadav/Datasets/Assignment2/training1/jeans/'+i)\n",
    "    height,width=image.shape[:2]\n",
    "    rotation_matrix=cv2.getRotationMatrix2D((width/2,height/2),-30,.5)\n",
    "    rotated_image=cv2.warpAffine(image, rotation_matrix,(width,height))\n",
    "    # in the below line corrected_code_augmented is the folder where we are saving the augmented images. so first create a directory \n",
    "    # like this and then the path as shown below.\n",
    "    cv2.imwrite('D:/fliprobo/augmented_rotated(-30)/jeans/'+'img{}.jpg'.format(j),rotated_image)\n",
    "    \n",
    "    \n",
    "l=[]\n",
    "for j,i in enumerate(onlyfiles):\n",
    "    image=cv2.imread('C:/Users/Shailendra Yadav/Datasets/Assignment2/training1/trouser/'+i)\n",
    "    height,width=image.shape[:2]\n",
    "    rotation_matrix=cv2.getRotationMatrix2D((width/2,height/2),-30,.5)\n",
    "    rotated_image=cv2.warpAffine(image, rotation_matrix,(width,height))\n",
    "    # in the below line corrected_code_augmented is the folder where we are saving the augmented images. so first create a directory \n",
    "    # like this and then the path as shown below.\n",
    "    cv2.imwrite('D:/fliprobo/augmented_rotated(-30)/trouser/'+'img{}.jpg'.format(j),rotated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "for j,i in enumerate(onlyfiles):\n",
    "    image=cv2.imread('C:/Users/Shailendra Yadav/Datasets/Assignment2/training1/jeans/'+i)\n",
    "    \n",
    "    M=np.ones(image.shape, dtype='uint8')*75\n",
    "    added=cv2.add(image,M)\n",
    "    # in the below line corrected_code_augmented is the folder where we are saving the augmented images. so first create a directory \n",
    "    # like this and then the path as shown below.\n",
    "    cv2.imwrite('D:/fliprobo/Brightness/jeans/'+'img{}.jpg'.format(j),added)\n",
    "    \n",
    "    \n",
    "l=[]\n",
    "for j,i in enumerate(onlyfiles):\n",
    "    image=cv2.imread('C:/Users/Shailendra Yadav/Datasets/Assignment2/training1/trouser/'+i)\n",
    "    \n",
    "    M=np.ones(image.shape, dtype='uint8')*75\n",
    "    added=cv2.add(image,M)\n",
    "    # in the below line corrected_code_augmented is the folder where we are saving the augmented images. so first create a directory \n",
    "    # like this and then the path as shown below.\n",
    "    cv2.imwrite('D:/fliprobo/Brightness/trouser/'+'img{}.jpg'.format(j),added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "for j,i in enumerate(onlyfiles):\n",
    "    image=cv2.imread('C:/Users/Shailendra Yadav/Datasets/Assignment2/training1/jeans/'+i)\n",
    "    \n",
    "    M=np.ones(image.shape, dtype='uint8')*75\n",
    "    subtracted=cv2.subtract(image,M)\n",
    "    # in the below line corrected_code_augmented is the folder where we are saving the augmented images. so first create a directory \n",
    "    # like this and then the path as shown below.\n",
    "    cv2.imwrite('D:/fliprobo/low_brightness/jeans/'+'img{}.jpg'.format(j),subtracted)\n",
    "    \n",
    "l=[]\n",
    "for j,i in enumerate(onlyfiles):\n",
    "    image=cv2.imread('C:/Users/Shailendra Yadav/Datasets/Assignment2/training1/trouser/'+i)\n",
    "    \n",
    "    M=np.ones(image.shape, dtype='uint8')*75\n",
    "    subtracted=cv2.subtract(image,M)\n",
    "    # in the below line corrected_code_augmented is the folder where we are saving the augmented images. so first create a directory \n",
    "    # like this and then the path as shown below.\n",
    "    cv2.imwrite('D:/fliprobo/low_brightness/trouser/'+'img{}.jpg'.format(j),subtracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "for j,i in enumerate(onlyfiles):\n",
    "    image=cv2.imread('C:/Users/Shailendra Yadav/Datasets/Assignment2/training1/jeans/'+i)\n",
    "    \n",
    "    flipped=cv2.flip(image,1)\n",
    "    # in the below line corrected_code_augmented is the folder where we are saving the augmented images. so first create a directory \n",
    "    # like this and then the path as shown below.\n",
    "    cv2.imwrite('D:/fliprobo/flipped/jeans/'+'img{}.jpg'.format(j),flipped)\n",
    "    \n",
    "l=[]\n",
    "for j,i in enumerate(onlyfiles):\n",
    "    image=cv2.imread('C:/Users/Shailendra Yadav/Datasets/Assignment2/training1/trouser/'+i)\n",
    "    \n",
    "    flipped=cv2.flip(image,1)\n",
    "    # in the below line corrected_code_augmented is the folder where we are saving the augmented images. so first create a directory \n",
    "    # like this and then the path as shown below.\n",
    "    cv2.imwrite('D:/fliprobo/flipped/trouser/'+'img{}.jpg'.format(j),flipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "for j,i in enumerate(onlyfiles):\n",
    "    image=cv2.imread('C:/Users/Shailendra Yadav/Datasets/Assignment2/training1/jeans/'+i)\n",
    "    \n",
    "    \n",
    "    kernel_sharpening=np.array([[-1,-1,-1],\n",
    "                            [-1,10,-1],\n",
    "                            [-1,-1,-1]])\n",
    "#applying different kernels to the input image\n",
    "    sharpened=cv2.filter2D(image,-1,kernel_sharpening)\n",
    "    # in the below line corrected_code_augmented is the folder where we are saving the augmented images. so first create a directory \n",
    "    # like this and then the path as shown below.\n",
    "    cv2.imwrite('D:/fliprobo/sharpened/jeans/'+'img{}.jpg'.format(j),sharpened)\n",
    "    \n",
    "    \n",
    "l=[]\n",
    "for j,i in enumerate(onlyfiles):\n",
    "    image=cv2.imread('C:/Users/Shailendra Yadav/Datasets/Assignment2/training1/trouser/'+i)\n",
    "    \n",
    "    \n",
    "    kernel_sharpening=np.array([[-1,-1,-1],\n",
    "                            [-1,10,-1],\n",
    "                            [-1,-1,-1]])\n",
    "#applying different kernels to the input image\n",
    "    sharpened=cv2.filter2D(image,-1,kernel_sharpening)\n",
    "    # in the below line corrected_code_augmented is the folder where we are saving the augmented images. so first create a directory \n",
    "    # like this and then the path as shown below.\n",
    "    cv2.imwrite('D:/fliprobo/sharpened/trouser/'+'img{}.jpg'.format(j),sharpened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=\"D:/fliprobo/training1/\"\n",
    "validation=\"D:/fliprobo/validation1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=(150,150,3)\n",
    "img_width=150\n",
    "img_height=150\n",
    "\n",
    "train_samples=490\n",
    "validation_samples=60\n",
    "\n",
    "batch_size=32\n",
    "num_classes=2\n",
    "epochs=20\n",
    "\n",
    "datagen=ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 980 images belonging to 2 classes.\n",
      "Found 60 images belonging to 2 classes.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\test\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 148, 148, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 72, 72, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 72, 72, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 34, 34, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 34, 34, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18496)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                1183808   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,212,513\n",
      "Trainable params: 1,212,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "train_generator=datagen.flow_from_directory(train,\n",
    "                                            target_size=(img_width,img_height),\n",
    "                                           batch_size=16,\n",
    "                                           class_mode='binary')\n",
    "validation_generator=datagen.flow_from_directory(validation,\n",
    "                                                target_size=(img_width,img_height),\n",
    "                                                batch_size=32,\n",
    "                                                class_mode='binary')\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Conv2D(32,(3,3),input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(32,(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(64,(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='rmsprop',\n",
    "             metrics=['accuracy'])\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\test\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/20\n",
      "15/15 [==============================] - 65s 4s/step - loss: 1.1077 - accuracy: 0.5351 - val_loss: 0.6935 - val_accuracy: 0.4688\n",
      "Epoch 2/20\n",
      "15/15 [==============================] - 65s 4s/step - loss: 0.6992 - accuracy: 0.5542 - val_loss: 0.6920 - val_accuracy: 0.7500\n",
      "Epoch 3/20\n",
      "15/15 [==============================] - 66s 4s/step - loss: 0.6929 - accuracy: 0.5333 - val_loss: 0.6928 - val_accuracy: 0.5312\n",
      "Epoch 4/20\n",
      "15/15 [==============================] - 65s 4s/step - loss: 0.6936 - accuracy: 0.5792 - val_loss: 0.6918 - val_accuracy: 0.6786\n",
      "Epoch 5/20\n",
      "15/15 [==============================] - 63s 4s/step - loss: 0.6613 - accuracy: 0.5877 - val_loss: 0.6905 - val_accuracy: 0.6250\n",
      "Epoch 6/20\n",
      "15/15 [==============================] - 65s 4s/step - loss: 0.6336 - accuracy: 0.6208 - val_loss: 0.6731 - val_accuracy: 0.5714\n",
      "Epoch 7/20\n",
      "15/15 [==============================] - 67s 4s/step - loss: 0.5851 - accuracy: 0.6875 - val_loss: 0.6819 - val_accuracy: 0.5312\n",
      "Epoch 8/20\n",
      "15/15 [==============================] - 65s 4s/step - loss: 0.5534 - accuracy: 0.7250 - val_loss: 0.6810 - val_accuracy: 0.6071\n",
      "Epoch 9/20\n",
      "15/15 [==============================] - 68s 5s/step - loss: 0.5877 - accuracy: 0.6917 - val_loss: 0.6972 - val_accuracy: 0.4062\n",
      "Epoch 10/20\n",
      "15/15 [==============================] - 65s 4s/step - loss: 0.5015 - accuracy: 0.7583 - val_loss: 0.6473 - val_accuracy: 0.6429\n",
      "Epoch 11/20\n",
      "15/15 [==============================] - 63s 4s/step - loss: 0.5194 - accuracy: 0.7456 - val_loss: 0.6746 - val_accuracy: 0.5938\n",
      "Epoch 12/20\n",
      "15/15 [==============================] - 65s 4s/step - loss: 0.4842 - accuracy: 0.8000 - val_loss: 0.6126 - val_accuracy: 0.6071\n",
      "Epoch 13/20\n",
      "15/15 [==============================] - 66s 4s/step - loss: 0.4169 - accuracy: 0.7833 - val_loss: 0.6312 - val_accuracy: 0.6250\n",
      "Epoch 14/20\n",
      "15/15 [==============================] - 65s 4s/step - loss: 0.4019 - accuracy: 0.8167 - val_loss: 0.6200 - val_accuracy: 0.6071\n",
      "Epoch 15/20\n",
      "15/15 [==============================] - 67s 4s/step - loss: 0.3866 - accuracy: 0.8083 - val_loss: 1.0353 - val_accuracy: 0.5938\n",
      "Epoch 16/20\n",
      "15/15 [==============================] - 63s 4s/step - loss: 0.4027 - accuracy: 0.8070 - val_loss: 0.5988 - val_accuracy: 0.5357\n",
      "Epoch 17/20\n",
      "15/15 [==============================] - 63s 4s/step - loss: 0.4076 - accuracy: 0.8202 - val_loss: 0.6239 - val_accuracy: 0.7500\n",
      "Epoch 18/20\n",
      "15/15 [==============================] - 67s 4s/step - loss: 0.4088 - accuracy: 0.8125 - val_loss: 0.7327 - val_accuracy: 0.6071\n",
      "Epoch 19/20\n",
      "15/15 [==============================] - 65s 4s/step - loss: 0.3530 - accuracy: 0.8167 - val_loss: 0.5650 - val_accuracy: 0.6562\n",
      "Epoch 20/20\n",
      "15/15 [==============================] - 65s 4s/step - loss: 0.3732 - accuracy: 0.8417 - val_loss: 0.6834 - val_accuracy: 0.7500\n"
     ]
    }
   ],
   "source": [
    "history=model.fit_generator(train_generator,\n",
    "                            steps_per_epoch=train_samples // batch_size,\n",
    "                 epochs=epochs,\n",
    "                            validation_data=validation_generator,\n",
    "                            validation_steps=validation_samples // batch_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Assignment3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dir_path='C:/Users/Shailendra Yadav/Datasets/Assignment2/test1/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['img_01.jpeg', 'img_02.jpeg', 'img_03.jpeg', 'img_101.jpeg', 'img_102.jpeg', 'img_103.jpeg', 'img_11.jpeg', 'img_12.jpeg', 'img_13.jpeg', 'img_21.jpeg', 'img_22.jpeg', 'img_23.jpeg', 'img_232.jpeg', 'img_233.jpeg', 'img_241.jpeg', 'img_242.jpeg', 'img_243.jpeg', 'img_251.jpeg', 'img_252.jpeg', 'img_253.jpeg', 'img_261.jpeg', 'img_262.jpeg', 'img_263.jpeg', 'img_271.jpeg', 'img_272.jpeg', 'img_273.jpeg', 'img_281.jpeg', 'img_282.jpeg', 'img_283.jpeg', 'img_291.jpeg', 'img_292.jpeg', 'img_293.jpeg', 'img_301.jpeg', 'img_302.jpeg', 'img_303.jpeg', 'img_31.jpeg', 'img_311.jpeg', 'img_312.jpeg', 'img_313.jpeg', 'img_32.jpeg', 'img_321.jpeg', 'img_322.jpeg', 'img_323.jpeg', 'img_33.jpeg', 'img_331.jpeg', 'img_41.jpeg', 'img_42.jpeg', 'img_43.jpeg', 'img_51.jpeg', 'img_52.jpeg', 'img_53.jpeg', 'img_61.jpeg', 'img_62.jpeg', 'img_63.jpeg', 'img_71.jpeg', 'img_72.jpeg', 'img_73.jpeg', 'img_81.jpeg', 'img_82.jpeg', 'img_83.jpeg', 'img_91.jpeg', 'img_92.jpeg', 'img_93.jpeg']\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "onlyfiles=[f for f in listdir(predict_dir_path) if isfile(join(predict_dir_path, f))]\n",
    "print(onlyfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_01.jpeg:jean\n",
      "img_02.jpeg:jean\n",
      "img_03.jpeg:trouser\n",
      "img_101.jpeg:jean\n",
      "img_102.jpeg:jean\n",
      "img_103.jpeg:jean\n",
      "img_11.jpeg:trouser\n",
      "img_12.jpeg:jean\n",
      "img_13.jpeg:jean\n",
      "img_21.jpeg:jean\n",
      "img_22.jpeg:jean\n",
      "img_23.jpeg:jean\n",
      "img_232.jpeg:trouser\n",
      "img_233.jpeg:trouser\n",
      "img_241.jpeg:trouser\n",
      "img_242.jpeg:trouser\n",
      "img_243.jpeg:trouser\n",
      "img_251.jpeg:jean\n",
      "img_252.jpeg:jean\n",
      "img_253.jpeg:jean\n",
      "img_261.jpeg:trouser\n",
      "img_262.jpeg:trouser\n",
      "img_263.jpeg:trouser\n",
      "img_271.jpeg:trouser\n",
      "img_272.jpeg:trouser\n",
      "img_273.jpeg:trouser\n",
      "img_281.jpeg:trouser\n",
      "img_282.jpeg:trouser\n",
      "img_283.jpeg:trouser\n",
      "img_291.jpeg:trouser\n",
      "img_292.jpeg:trouser\n",
      "img_293.jpeg:trouser\n",
      "img_301.jpeg:jean\n",
      "img_302.jpeg:jean\n",
      "img_303.jpeg:jean\n",
      "img_31.jpeg:jean\n",
      "img_311.jpeg:jean\n",
      "img_312.jpeg:jean\n",
      "img_313.jpeg:jean\n",
      "img_32.jpeg:jean\n",
      "img_321.jpeg:jean\n",
      "img_322.jpeg:jean\n",
      "img_323.jpeg:jean\n",
      "img_33.jpeg:jean\n",
      "img_331.jpeg:jean\n",
      "img_41.jpeg:jean\n",
      "img_42.jpeg:jean\n",
      "img_43.jpeg:jean\n",
      "img_51.jpeg:jean\n",
      "img_52.jpeg:jean\n",
      "img_53.jpeg:jean\n",
      "img_61.jpeg:jean\n",
      "img_62.jpeg:jean\n",
      "img_63.jpeg:jean\n",
      "img_71.jpeg:jean\n",
      "img_72.jpeg:jean\n",
      "img_73.jpeg:jean\n",
      "img_81.jpeg:jean\n",
      "img_82.jpeg:jean\n",
      "img_83.jpeg:jean\n",
      "img_91.jpeg:jean\n",
      "img_92.jpeg:jean\n",
      "img_93.jpeg:jean\n",
      "total jeans: 44\n",
      "total trousers: 19\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "jean_counter=0\n",
    "trouser_counter=0\n",
    "for file in onlyfiles:\n",
    "    img=image.load_img(predict_dir_path + file,target_size=(img_width,img_height))\n",
    "    x=image.img_to_array(img)\n",
    "    x=np.expand_dims(x, axis=0)\n",
    "    \n",
    "    images=np.vstack([x])\n",
    "    classes=model.predict_classes(images,batch_size=10)\n",
    "    classes=classes[0][0]\n",
    "    \n",
    "    if classes==0:\n",
    "        print(file+\":\"+'jean')\n",
    "        jean_counter+=1\n",
    "    else:\n",
    "        print(file+\":\"+'trouser')\n",
    "        trouser_counter+=1\n",
    "print(\"total jeans:\",jean_counter)\n",
    "print(\"total trousers:\",trouser_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img_to_array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-114-f121282c98b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mpredict_dir_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict_dir_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Users/Shailendra Yadav/Assignment3.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'img_to_array' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.preprocessing import image\n",
    "predict_dir_path=img_to_array(predict_dir_path)\n",
    "\n",
    "classifier=load_model(\"C:/Users/Shailendra Yadav/Assignment3.h5\")\n",
    "\n",
    "def draw_test(name, pred, input_im):\n",
    "    BLACK=[0,0,0]\n",
    "    if pred==\"[0]\":\n",
    "        pred=\"jeans\"\n",
    "    if pred==\"[1]\":\n",
    "        pred=\"trouser\"\n",
    "    expanded_image=cv2.copyMakeBorder(input_im, 0, 0, 0, imageL.shape[0],cv2.BORDER_CONSTANT,value=BLACK)\n",
    "    \n",
    "    cv2.putText(expanded_image,str(pred),(252,70),cv2.FONT_HERSHEY_COMPLES_SMALL,4,(0,255,0),2)\n",
    "    cv2.imshow(name,expanded_image)\n",
    "    \n",
    "for i in range (0,10):\n",
    "    rand=np.random.randint(0,len(predict_dir_path))\n",
    "    input_im=predict_dir_path[rand]\n",
    "    \n",
    "    imageL=cv2.resize(input_im, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n",
    "    cv2.imshow(\"Test Image\", imageL)\n",
    "    \n",
    "    input_im=input_im.reshape(1,150,150,3)\n",
    "    \n",
    "    ##get prediction\n",
    "    res=str(classifier.predict_classes(input_im,1,verbose=0)[0])\n",
    "    \n",
    "    draw_test(\"prediction\",res,imageL)\n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
