{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word tokenization\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "text='I am learning Natural Language processing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Shailendra\n",
      "[nltk_data]     Yadav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'learning', 'Natural', 'Language', 'processing']\n"
     ]
    }
   ],
   "source": [
    "#tokenizing\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am learning Natural Language processing']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good.', 'Mrning!', 'How are you?.']\n"
     ]
    }
   ],
   "source": [
    "text='Good. Mrning! How are you?.'\n",
    "#text='Good Morning! How are you'\n",
    "#ext=\"our company annual growth rate is 25.50%. Good job Mr.Bajaj\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REGULAR EXPRESSIONS\n",
    "from nltk.tokenize import regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_text\n",
    "text=\"NLP is fun and Can deal with texts and sounds, but can't deal with Images. We have a session at 11AM!. we can earn a lot of $\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is',\n",
       " 'fun',\n",
       " 'and',\n",
       " 'an',\n",
       " 'deal',\n",
       " 'with',\n",
       " 'texts',\n",
       " 'and',\n",
       " 'sounds',\n",
       " 'but',\n",
       " 'can',\n",
       " 't',\n",
       " 'deal',\n",
       " 'with',\n",
       " 'mages',\n",
       " 'e',\n",
       " 'have',\n",
       " 'a',\n",
       " 'session',\n",
       " 'at',\n",
       " 'we',\n",
       " 'can',\n",
       " 'earn',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print all words that contain small case and starts from small a-z\n",
    "regexp_tokenize(text,'[a-z]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLP', 'C', 'I', 'W', 'AM']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print word by word that contains all caps and from caps A to z\n",
    "regexp_tokenize(text,\"[A-Z]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is',\n",
       " 'fun',\n",
       " 'and',\n",
       " 'an',\n",
       " 'deal',\n",
       " 'with',\n",
       " 'texts',\n",
       " 'and',\n",
       " 'sounds',\n",
       " 'but',\n",
       " \"can't\",\n",
       " 'deal',\n",
       " 'with',\n",
       " 'mages',\n",
       " 'e',\n",
       " 'have',\n",
       " 'a',\n",
       " 'session',\n",
       " 'at',\n",
       " 'we',\n",
       " 'can',\n",
       " 'earn',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extra quote gets you words like can't, don't etc\n",
    "regexp_tokenize(text,\"[a-z']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"NLP is fun and Can deal with texts and sounds, but can't deal with Images. We have a session at 11AM!. we can earn a lot of $\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#everything in one line\n",
    "regexp_tokenize(text,\"[\\a-z']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLP ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' C',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ', ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' I',\n",
       " '. W',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' 11AM!. ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' $']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#anything starts with caret is not equal.\n",
    "regexp_tokenize(text,\"[^a-z']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['11']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only numbers\n",
    "regexp_tokenize(text,\"[0-9]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"NLP is fun and Can deal with texts and sounds, but can't deal with Images. We have a session at \",\n",
       " 'AM!. we can earn a lot of $']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Without numbers\n",
    "regexp_tokenize(text,\"[^0-9]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to extract specific character\n",
    "regexp_tokenize(text,\"[$]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Shailendra Yadav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#POS (Part of speech)\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('We', 'PRP'),\n",
       " ('will', 'MD'),\n",
       " ('see', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('example', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('POS', 'NNP'),\n",
       " ('tagging', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data='We will see a example of POS tagging.'\n",
    "\n",
    "pos=nltk.pos_tag(nltk.word_tokenize(data))\n",
    "pos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  We/PRP\n",
      "  will/MD\n",
      "  see/VB\n",
      "  an/DT\n",
      "  (MN example/NN)\n",
      "  of/IN\n",
      "  (MN pos/NN)\n",
      "  (MN tagging/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "#CHUNKING\n",
    "#After using parts of speech, chunking can be used to make data more structured by giving a specefic set of rules.let understand more about chunking by following example.\n",
    "data='We will see an example of pos tagging.'\n",
    "\n",
    "pos=nltk.pos_tag(nltk.word_tokenize(data))\n",
    "\n",
    "#now once the POS tag has been done. let's say we want to further structure data such that nouns are\n",
    "#categorised under one specefic node defined bby us:\n",
    "\n",
    "my_node=\"MN:{<NNP>*<NN>}\"\n",
    "\n",
    "chunk=nltk.RegexpParser(my_node)\n",
    "\n",
    "result=chunk.parse(pos)\n",
    "\n",
    "print(result)\n",
    "#result.draw() # this generates a graphical picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Shailendra\n",
      "[nltk_data]     Yadav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#STOP WORDS\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another way\n",
    "stopset=set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adding custom stopwords\n",
    "stopset.update(('new','wonder'))\n",
    "len(stopset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SIMILAR TO STOPWORDS, WE CAN ALSO IGNORE PUNCTUATIONS IN OUR SENTENCES.\n",
    "\n",
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwods and punctuations from text \n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words=stopwords.words('english')\n",
    "punct=string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"India (Hindi: Bhārat), officially the Republic of India, is a country in South Asia. It is the seventh-largest country by area, the second-most populous country, and the most populous democracy in the world. Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west; China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east. In the Indian Ocean, India is in the vicinity of Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Thailand and Indonesia.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original length==> 614\n",
      "length of cleaned text==> 57\n",
      "\n",
      " ['India', 'Hindi', 'Bhārat', 'officially', 'Republic', 'India', 'country', 'South', 'Asia', 'It', 'seventh-largest', 'country', 'area', 'second-most', 'populous', 'country', 'populous', 'democracy', 'world', 'Bounded', 'Indian', 'Ocean', 'south', 'Arabian', 'Sea', 'southwest', 'Bay', 'Bengal', 'southeast', 'shares', 'land', 'borders', 'Pakistan', 'west', 'China', 'Nepal', 'Bhutan', 'north', 'Bangladesh', 'Myanmar', 'east', 'In', 'Indian', 'Ocean', 'India', 'vicinity', 'Sri', 'Lanka', 'Maldives', 'Andaman', 'Nicobar', 'Islands', 'share', 'maritime', 'border', 'Thailand', 'Indonesia']\n"
     ]
    }
   ],
   "source": [
    "#empty list to add clean data\n",
    "\n",
    "cleaned_text=[]\n",
    "\n",
    "for word in nltk.word_tokenize(text):\n",
    "    if word not in punct:\n",
    "        if word not in stop_words:\n",
    "            cleaned_text.append(word)\n",
    "            \n",
    "print('original length==>',len(text))\n",
    "print('length of cleaned text==>',len(cleaned_text))\n",
    "print('\\n',cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "india (hindi: bhārat), officially the republic of india, is a country in south asia. it is the seventh-largest country by area, the second-most populous country, and the most populous democracy in the world. bounded by the indian ocean on the south, the arabian sea on the southwest, and the bay of bengal on the southeast, it shares land borders with pakistan to the west; china, nepal, and bhutan to the north; and bangladesh and myanmar to the east. in the indian ocean, india is in the vicinity of sri lanka and the maldives; its andaman and nicobar islands share a maritime border with thailand and indonesia.\n"
     ]
    }
   ],
   "source": [
    "#cases\n",
    "print(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDIA (HINDI: BHĀRAT), OFFICIALLY THE REPUBLIC OF INDIA, IS A COUNTRY IN SOUTH ASIA. IT IS THE SEVENTH-LARGEST COUNTRY BY AREA, THE SECOND-MOST POPULOUS COUNTRY, AND THE MOST POPULOUS DEMOCRACY IN THE WORLD. BOUNDED BY THE INDIAN OCEAN ON THE SOUTH, THE ARABIAN SEA ON THE SOUTHWEST, AND THE BAY OF BENGAL ON THE SOUTHEAST, IT SHARES LAND BORDERS WITH PAKISTAN TO THE WEST; CHINA, NEPAL, AND BHUTAN TO THE NORTH; AND BANGLADESH AND MYANMAR TO THE EAST. IN THE INDIAN OCEAN, INDIA IS IN THE VICINITY OF SRI LANKA AND THE MALDIVES; ITS ANDAMAN AND NICOBAR ISLANDS SHARE A MARITIME BORDER WITH THAILAND AND INDONESIA.\n"
     ]
    }
   ],
   "source": [
    "print(text.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEMMING\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer,SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer\n",
      "hobbi\n",
      "hobbi\n",
      "comput\n",
      "comput\n",
      "*******************\n",
      "Lancaster stemmer\n",
      "hobby\n",
      "hobby\n",
      "comput\n",
      "comput\n",
      "*******************************\n"
     ]
    }
   ],
   "source": [
    "lancaster=LancasterStemmer()\n",
    "porter=PorterStemmer()\n",
    "Snowball=SnowballStemmer('english')\n",
    "\n",
    "print('Porter Stemmer')\n",
    "print(porter.stem(\"hobby\"))\n",
    "print(porter.stem(\"hobbies\"))\n",
    "print(porter.stem(\"computer\"))\n",
    "print(porter.stem(\"computation\"))\n",
    "print('*******************')\n",
    "\n",
    "print('Lancaster stemmer')\n",
    "print(lancaster.stem(\"hobby\"))\n",
    "print(lancaster.stem('hobbies'))\n",
    "print(lancaster.stem('computer'))\n",
    "print(lancaster.stem('computation'))\n",
    "print('*******************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i was go to the offic on my bicycl when i saw a car pass by hit th tree\n",
      "i was going to the off on my bicyc when i saw a car pass by hit th tre\n",
      "i wa go to the offic on my bicycl when i saw a car pass by hit th tree\n"
     ]
    }
   ],
   "source": [
    "sentence=\"i was going to the office on my bicycle when i saw a car passing by hit th tree\"\n",
    "\n",
    "token=list(nltk.word_tokenize(sentence))\n",
    "\n",
    "for stemmer in (Snowball,lancaster,porter):\n",
    "    stemm=[stemmer.stem(t) for t in token]\n",
    "    print(\" \".join(stemm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Shailendra\n",
      "[nltk_data]     Yadav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "run\n",
      "ran\n"
     ]
    }
   ],
   "source": [
    "#LEMMATIZATION\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemma=WordNetLemmatizer()\n",
    "\n",
    "print(lemma.lemmatize('running'))\n",
    "print(lemma.lemmatize('runs'))\n",
    "print(lemma.lemmatize('ran'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemming of Bring is bring\n",
      "stemming of King is king\n",
      "stemming of Going is go\n",
      "stemming of Anything is anyth\n",
      "stemming of Sing is sing\n",
      "stemming of Ring is ring\n",
      "stemming of Nothing is noth\n",
      "stemming of Thing is thing\n"
     ]
    }
   ],
   "source": [
    "#one more example\n",
    "\n",
    "text=\"Bring King Going Anything Sing Ring Nothing Thing\"\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer=PorterStemmer()\n",
    "tokenization=nltk.word_tokenize(text)\n",
    "\n",
    "for w in tokenization:\n",
    "    print(\"stemming of {} is {}\".format(w,porter_stemmer.stem(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for Bring is Bring\n",
      "Lemma for King is King\n",
      "Lemma for Going is Going\n",
      "Lemma for Anything is Anything\n",
      "Lemma for Sing is Sing\n",
      "Lemma for Ring is Ring\n",
      "Lemma for Nothing is Nothing\n",
      "Lemma for Thing is Thing\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokenization = nltk.word_tokenize(text)\n",
    "\n",
    "for w in tokenization:\n",
    "    print(\"Lemma for {} is {}\".format(w, wordnet_lemmatizer.lemmatize(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORDNET\n",
    "\n",
    "from nltk.corpus import wordnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('cat.n.01'), Synset('guy.n.01'), Synset('cat.n.03'), Synset('kat.n.01'), Synset('cat-o'-nine-tails.n.01'), Synset('caterpillar.n.02'), Synset('big_cat.n.01'), Synset('computerized_tomography.n.01'), Synset('cat.v.01'), Synset('vomit.v.01')]\n"
     ]
    }
   ],
   "source": [
    "#synset is also called as synonym set or collection of synonym words.\n",
    "#Let us check an example\n",
    "\n",
    "syns=wordnet.synsets(\"cat\")\n",
    "print(syns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('dog.n.01'), Synset('frump.n.01'), Synset('dog.n.03'), Synset('cad.n.01'), Synset('frank.n.02'), Synset('pawl.n.01'), Synset('andiron.n.01'), Synset('chase.v.01')]\n"
     ]
    }
   ],
   "source": [
    "syns=wordnet.synsets(\"dog\")\n",
    "print(syns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms => {'fighting', 'dynamic', 'participating', 'alive', 'active', 'active_agent', 'active_voice', 'combat-ready'}\n",
      "Antonyms => {'inactive', 'passive', 'passive_voice', 'extinct', 'stative', 'quiet', 'dormant'}\n"
     ]
    }
   ],
   "source": [
    "# Lets find sysnonms and antonyms using python code\n",
    "from nltk.corpus import wordnet\n",
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(\"active\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "\n",
    "print('Synonyms =>',set(synonyms))\n",
    "print('Antonyms =>',set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
